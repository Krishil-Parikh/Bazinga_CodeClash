{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9306149,"sourceType":"datasetVersion","datasetId":5635372}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-08T10:51:37.134381Z","iopub.execute_input":"2024-09-08T10:51:37.134881Z","iopub.status.idle":"2024-09-08T10:51:37.542821Z","shell.execute_reply.started":"2024-09-08T10:51:37.134821Z","shell.execute_reply":"2024-09-08T10:51:37.541568Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/lin-reg/lin-reg-from-scratch.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Linear Regression Model¶\n\nLinear regression is a fundamental model in machine learning used for predicting a continuous output variable based on input features. The model function for linear regression is represented as:\n\nf\nw\n,\nb\n(\nx\n)\n=\nw\nx\n+\nb\n\nIn this equation, \nf\nw\n,\nb\n(\nx\n)\n represents the predicted output, \nw\n is the weight parameter, \nb\n is the bias parameter, and \nx\n is the input feature.\n\nModel Training\nTo train a linear regression model, we aim to find the best values for the parameters \n(\nw\n,\nb\n)\n that best fit our dataset.\n\nForward Pass\nThe forward pass is a step where we compute the linear regression output for the input data \nX\n using the current weights and biases. It's essentially applying our model to the input data.\n\nCost Function\nThe cost function is used to measure how well our model is performing. It quantifies the difference between the predicted values and the actual values in our dataset. The cost function is defined as:\n\nJ\n(\nw\n,\nb\n)\n=\n1\n2\nm\nm\n∑\ni\n=\n1\n \n(\nf\nw\n,\nb\n(\nx\n(\ni\n)\n)\n−\ny\n(\ni\n)\n)\n2\nHere, \nJ\n(\nw\n,\nb\n)\n is the cost, \nm\n is the number of training examples, \nx\n(\ni\n)\n is the input data for the \ni\n-th example, \ny\n(\ni\n)\n is the actual output for the \ni\n-th example, and \nw\n and \nb\n are the weight and bias parameters, respectively.\n\nBackward Pass (Gradient Computation)\nThe backward pass computes the gradients of the cost function with respect to the weights and biases. These gradients are crucial for updating the model parameters during training. The gradient formulas are as follows:\n\n∂\nJ\n(\nw\n,\nb\n)\n∂\nb\n=\n1\nm\nm\n−\n1\n∑\ni\n=\n0\n \n(\nf\nw\n,\nb\n(\nX\n(\ni\n)\n)\n−\ny\n(\ni\n)\n)\n∂\nJ\n(\nw\n,\nb\n)\n∂\nw\n=\n1\nm\nm\n−\n1\n∑\ni\n=\n0\n \n(\nf\nw\n,\nb\n(\nX\n(\ni\n)\n)\n−\ny\n(\ni\n)\n)\nX\n(\ni\n)\nTraining Process\nThe training process involves iteratively updating the weights and biases to minimize the cost function. This is typically done through an optimization algorithm like gradient descent. The update equations for parameters are:\n\nw\n←\nw\n−\nα\n∂\nJ\n∂\nw\nb\n←\nb\n−\nα\n∂\nJ\n∂\nb\nHere, \nα\n represents the learning rate, which controls the step size during parameter updates.\n\nBy iteratively performing the forward pass, computing the cost, performing the backward pass, and updating the parameters, the model learns to make better predictions and fit the data.","metadata":{}},{"cell_type":"code","source":"pd1=pd.read_csv('/kaggle/input/lin-reg/lin-reg-from-scratch.csv')\npd1","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:06:47.203389Z","iopub.execute_input":"2024-09-03T06:06:47.204353Z","iopub.status.idle":"2024-09-03T06:06:47.235904Z","shell.execute_reply.started":"2024-09-03T06:06:47.204305Z","shell.execute_reply":"2024-09-03T06:06:47.234695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearRegression:\n    \n    #Linear Regression Model with Gradient Descent\n\n    #Linear regression is a supervised machine learning algorithm used for modeling the relationship\n    between a dependent variable (target) and one or more independent variables (features) by fitting\n    a linear equation to the observed data.\n\n    #This class implements a linear regression model using gradient descent optimization for training.It provides methods for model initialization, training, prediction, and model persistence.\n\n    #Parameters:\n        #learning_rate (float): The learning rate used in gradient descent.\n        #convergence_tol (float, optional): The tolerance for convergence (stopping criterion). Defaults to 1e-6.\n\n    #Attributes:\n        #W (numpy.ndarray): Coefficients (weights) for the linear regression model.\n        #b (float): Intercept (bias) for the linear regression model.\n\n    #Methods:\n     #   initialize_parameters(n_features): Initialize model parameters.\n      #  forward(X): Compute the forward pass of the linear regression model.\n       # compute_cost(predictions): Compute the mean squared error cost.\n       # backward(predictions): Compute gradients for model parameters.\n      #  fit(X, y, iterations, plot_cost=True): Fit the linear regression model to training data.\n     #   predict(X): Predict target values for new input data.\n    #    save_model(filename=None): Save the trained model to a file using pickle.\n   #     load_model(filename): Load a trained model from a file using pickle.\n\n    #Examples:\n     #   >>> from linear_regression import LinearRegression\n      #  >>> model = LinearRegression(learning_rate=0.01)\n       # >>> model.fit(X_train, y_train, iterations=1000)\n        #>>> predictions = model.predict(X_test))\n   \n\n    def __init__(self, learning_rate, convergence_tol=1e-6):\n        self.learning_rate = learning_rate\n        self.convergence_tol = convergence_tol\n        self.W = None\n        self.b = None\n\n    def initialize_parameters(self, n_features):\n       \n        #Initialize model parameters.\n\n\n        self.W = np.random.randn(n_features) * 0.01\n        self.b = 0\n\n    def forward(self, X):\n    \n        #Compute the forward pass of the linear regression model.\n\n        #Parameters:\n            #X (numpy.ndarray): Input data of shape (m, n_features).\n\n        #Returns:\n         #   numpy.ndarray: Predictions of shape (m,).\n        \n        return np.dot(X, self.W) + self.b\n\n    def compute_cost(self, predictions):\n        \"\"\"\n        Compute the mean squared error cost.\n\n        Parameters:\n            predictions (numpy.ndarray): Predictions of shape (m,).\n\n        Returns:\n            float: Mean squared error cost.\n        \"\"\"\n        m = len(predictions)\n        cost = np.sum(np.square(predictions - self.y)) / (2 * m)\n        return cost\n\n    def backward(self, predictions):\n        \"\"\"\n        Compute gradients for model parameters.\n\n        Parameters:\n            predictions (numpy.ndarray): Predictions of shape (m,).\n\n        Updates:\n            numpy.ndarray: Gradient of W.\n            float: Gradient of b.\n        \"\"\"\n        m = len(predictions)\n        self.dW = np.dot(predictions - self.y, self.X) / m\n        self.db = np.sum(predictions - self.y) / m\n\n    def fit(self, X, y, iterations, plot_cost=True):\n        \"\"\"\n        Fit the linear regression model to the training data.\n\n        Parameters:\n            X (numpy.ndarray): Training input data of shape (m, n_features).\n            y (numpy.ndarray): Training labels of shape (m,).\n            iterations (int): The number of iterations for gradient descent.\n            plot_cost (bool, optional): Whether to plot the cost during training. Defaults to True.\n\n        Raises:\n            AssertionError: If input data and labels are not NumPy arrays or have mismatched shapes.\n\n        Plots:\n            Plotly line chart showing cost vs. iteration (if plot_cost is True).\n        \"\"\"\n        assert isinstance(X, np.ndarray), \"X must be a NumPy array\"\n        assert isinstance(y, np.ndarray), \"y must be a NumPy array\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n        assert iterations > 0, \"Iterations must be greater than 0\"\n\n        self.X = X\n        self.y = y\n        self.initialize_parameters(X.shape[1])\n        costs = []\n\n        for i in range(iterations):\n            predictions = self.forward(X)\n            cost = self.compute_cost(predictions)\n            self.backward(predictions)\n            self.W -= self.learning_rate * self.dW\n            self.b -= self.learning_rate * self.db\n            costs.append(cost)\n\n            if i % 100 == 0:\n                print(f'Iteration: {i}, Cost: {cost}')\n\n            if i > 0 and abs(costs[-1] - costs[-2]) < self.convergence_tol:\n                print(f'Converged after {i} iterations.')\n                break\n\n        if plot_cost:\n            fig = px.line(y=costs, title=\"Cost vs Iteration\", template=\"plotly_dark\")\n            fig.update_layout(\n                title_font_color=\"#41BEE9\",\n                xaxis=dict(color=\"#41BEE9\", title=\"Iterations\"),\n                yaxis=dict(color=\"#41BEE9\", title=\"Cost\")\n            )\n\n            fig.show()\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values for new input data.\n\n        Parameters:\n            X (numpy.ndarray): Input data of shape (m, n_features).\n\n        Returns:\n            numpy.ndarray: Predicted target values of shape (m,).\n        \"\"\"\n        return self.forward(X)\n    \n\n    def save_model(self, filename=None):\n        \"\"\"\n        Save the trained model to a file using pickle.\n\n        Parameters:\n            filename (str): The name of the file to save the model to.\n        \"\"\"\n        model_data = {\n            'learning_rate': self.learning_rate,\n            'convergence_tol': self.convergence_tol,\n            'W': self.W,\n            'b': self.b\n        }\n\n        with open(filename, 'wb') as file:\n            pickle.dump(model_data, file)\n\n    @classmethod\n    def load_model(cls, filename):\n        \"\"\"\n        Load a trained model from a file using pickle.\n\n        Parameters:\n            filename (str): The name of the file to load the model from.\n\n        Returns:\n            LinearRegression: An instance of the LinearRegression class with loaded parameters.\n        \"\"\"\n        with open(filename, 'rb') as file:\n            model_data = pickle.load(file)\n\n        # Create a new instance of the class and initialize it with the loaded parameters\n        loaded_model = cls(model_data['learning_rate'], model_data['convergence_tol'])\n        loaded_model.W = model_data['W']\n        loaded_model.b = model_data['b']\n\n        return loaded_model","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:46:01.662830Z","iopub.execute_input":"2024-09-08T10:46:01.663320Z","iopub.status.idle":"2024-09-08T10:46:01.722774Z","shell.execute_reply.started":"2024-09-08T10:46:01.663262Z","shell.execute_reply":"2024-09-08T10:46:01.721236Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"lr_scratch=LinearRegression(0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:50:53.664189Z","iopub.execute_input":"2024-09-08T10:50:53.664695Z","iopub.status.idle":"2024-09-08T10:50:53.671045Z","shell.execute_reply.started":"2024-09-08T10:50:53.664653Z","shell.execute_reply":"2024-09-08T10:50:53.669649Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"lr_scratch.save_model('model_lr.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:51:42.482415Z","iopub.execute_input":"2024-09-08T10:51:42.483103Z","iopub.status.idle":"2024-09-08T10:51:42.489249Z","shell.execute_reply.started":"2024-09-08T10:51:42.483054Z","shell.execute_reply":"2024-09-08T10:51:42.488107Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}